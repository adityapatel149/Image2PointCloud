{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92313f0d-147f-4504-b8af-4df0f3b249e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install open3d --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5137a6c7-0137-4240-9e36-8aa5b51f2396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "#from transformers import GLPNImageProcessor, GLPNForDepthEstimation\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cea154-9b31-43f9-b2e7-566f2bfc9b7d",
   "metadata": {},
   "source": [
    "## Get Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a319919-4825-4672-9660-af395ebe268e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = AutoImageProcessor.from_pretrained(\"depth-anything/Depth-Anything-V2-Large-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"depth-anything/Depth-Anything-V2-Large-hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb3c43-ad82-47e9-a9e5-5588d0c0032a",
   "metadata": {},
   "source": [
    "## Image Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b71cd4c-5c0c-4473-9831-7cfa4155a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"test (15).jpg\"\n",
    "path = f\"./data/{filename}\"\n",
    "\n",
    "orig_image = Image.open(path)\n",
    "new_height = 720 if image.height > 720 else image.height\n",
    "new_height -= (new_height % 32)\n",
    "new_width = int(new_height* image.width/image.height)\n",
    "diff = new_width % 32\n",
    "\n",
    "new_width = new_width - diff if diff < 16 else new_width + 32 - diff\n",
    "new_size=(new_width, new_height)\n",
    "image = orig_image.resize(new_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cd4a680-4f80-4cc6-891a-a2910a6b5408",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Preparing image for model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m feature_extractor(images\u001b[38;5;241m=\u001b[39m\u001b[43mimage\u001b[49m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "# Preparing image for model\n",
    "inputs = feature_extractor(images=image, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d371e5d9-98cf-46ab-9779-2e72036ee624",
   "metadata": {},
   "source": [
    "## Getting prediction from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc96f2-9467-48b2-9331-fcfea22c7f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb05bb-fdd2-4081-9a91-467b0001f697",
   "metadata": {},
   "source": [
    "## Post-processing Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b908f8e-fe20-4919-b05f-8372c9315133",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processed_output = feature_extractor.post_process_depth_estimation(\n",
    "    outputs,\n",
    "    target_sizes=[(image.height, image.width)],\n",
    ")\n",
    "predicted_depth = post_processed_output[0][\"predicted_depth\"]\n",
    "output = predicted_depth.detach().cpu().numpy() * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1afbb4d-040b-41df-87cf-b2a8ae635daa",
   "metadata": {},
   "source": [
    "## Display output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1af7a-ff77-48d0-88c2-682a25dffc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig ,ax = plt.subplots(1,2)\n",
    "#ax[0].imshow(image)\n",
    "#ax[1].imshow(output, cmap='plasma')\n",
    "#plt.tight_layout()\n",
    "#plt.pause(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b44b691-3199-4d70-a2c6-72552d0c5afe",
   "metadata": {},
   "source": [
    "## Prepare depth image for open3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6563f4-68d2-4f98-9e31-64538f4f6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = image.size\n",
    "depth_image = ((output - output.min()) * 255 / (output.max() - output.min())) # keep values in 0,255\n",
    "depth_image = depth_image.max() - depth_image  # Invert depth values, because depthanything model gives higher value to close objects and lower to far. Point cloud is inverted if this step not done.\n",
    "\n",
    "threshold = 125 # points below the threshold will be pushed back\n",
    "lower_bound = 75 # minimum depth to avoid convergence of nearby pixels towards principle center\n",
    "#Essentially, we want the values below 125 to gradually increase towards 125, starting from 75. closer it is to 125, lesser it will be pushed back. (threshold - 75) is basically a scale of how much to push back.\n",
    "depth_image[depth_image < threshold] = (depth_image[depth_image < threshold] / threshold) * (threshold - 75) + 75\n",
    " \n",
    "# Ensure that no value goes above 255 (in case of overflow)\n",
    "depth_image = np.clip(depth_image, 0, 255)\n",
    "\n",
    "image_np = np.array(image)\n",
    "\n",
    "# Create rgbd image\n",
    "depth_o3d = o3d.geometry.Image(depth_image)\n",
    "#depth_o3d = o3d.geometry.Image(scaled_depth_map)\n",
    "image_o3d = o3d.geometry.Image(image_np)\n",
    "rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(image_o3d, depth_o3d, convert_rgb_to_intensity=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5088a7b-3261-4503-9c2c-4e24991f5ae0",
   "metadata": {},
   "source": [
    "## Setup camera setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8fc610-a1bb-44dd-b8f6-0557bebb2ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_intrinsic = o3d.camera.PinholeCameraIntrinsic()\n",
    "camera_intrinsic.set_intrinsics(width, height, 1000,1000, width/2, height/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff295be-f58a-4ccc-bfa5-29a8794faef3",
   "metadata": {},
   "source": [
    "## Create o3d point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acb6992-f48a-491a-bfac-6c2130fce306",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, camera_intrinsic)\n",
    "#o3d.visualization.draw_geometries([raw_pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58282cf5-5065-4939-9aba-947a58eb617b",
   "metadata": {},
   "source": [
    "## Post-process point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4130b9-5882-46cc-8283-c56adebbd7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "cl, ind = raw_pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=10.0)\n",
    "pcd = raw_pcd.select_by_index(ind)\n",
    "\n",
    "# Estimate normals\n",
    "pcd.estimate_normals()\n",
    "pcd.orient_normals_to_align_with_direction()\n",
    "#o3d.visualization.draw_geometries([pcd])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bcaea6-441e-4d0e-841c-fd2782a3ce26",
   "metadata": {},
   "source": [
    "## Surface reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "e4021d8b-b3ab-4d9d-ade4-8a28e6abd4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: The requested transformation operation is not supported. \n",
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: The requested transformation operation is not supported. \n",
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: The handle is invalid. \n",
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: The requested transformation operation is not supported. \n"
     ]
    }
   ],
   "source": [
    "mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=12, n_threads=4)\n",
    "\n",
    "# rotate mesh\n",
    "rotation = mesh.get_rotation_matrix_from_xyz((np.pi,0,0))\n",
    "mesh.rotate(rotation, center=(0,0,0))\n",
    "\n",
    "# remove vertices with low densities\n",
    "vertices_to_remove = densities < np.quantile(densities, 0.1)\n",
    "mesh.remove_vertices_by_mask(vertices_to_remove)\n",
    "\n",
    "# visualize the mesh\n",
    "o3d.visualization.draw_geometries([mesh], mesh_show_back_face=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "b42a237f-9ff6-44bb-ab8f-ae7a5a4641bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute nearest neighbor distances to determine pivot radii\n",
    "#distances = pcd.compute_nearest_neighbor_distance()\n",
    "#avg_dist = np.mean(distances)\n",
    "#radii = [avg_dist * factor for factor in [1, 4, 9]]  # Adjust factors as needed\n",
    "\n",
    "# Run BPA\n",
    "#bpa_mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_ball_pivoting( pcd, o3d.utility.DoubleVector(radii))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "a4166a8d-ec8d-45f6-b3f8-0f7f73698e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o3d.io.write_triangle_mesh(f\"./meshes/{filename}.ply\", mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb99b10e-cf1c-4a02-a3a9-00f2384dbfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([mesh], mesh_show_back_face=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a4df0d-8327-4242-8f1e-26be5223d5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49471191-21ca-45ba-b078-295807928479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
